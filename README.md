SHREY CHANDRA
Address: E-13.3, NBCC Vibgyor Towers,
New Town, Kolkata- 700059, West Bengal, India
Email: : shrey.2012.chandra@gmail.com
Cellular: +91 8463819595
Date of Birth: 14th October 1993


Work Experience
Programmer Analyst Cognizant Technology Solutions, Kolkata, West Bengal, India - Nov. 2016 onwards
overview:
•   Having 2.5 years’ experience in IT Industry with expertise in Database and Big Data technologies.
•   Active engagement in Projects using Hadoop ecosystems (HDFS, MapReduce, Yarn, PySpark, Hive, Sqoop, Oozie, Flume, Pig)
 	o   Strong understanding of Hadoop Architecture and various components such as HDFS, SparkContext, DriverProgram, Executor, Resource Manager, Application Master, Node Manager, Name Node & Data Node.
•	Created Shell scripts to automate metadata capturing of different DB sources.
 	o   Developed shell script to Run jobs from a Non-Hadoop linux server to remote Hadoop cluster using webhdfs API.

Projects

5/3rd Bancorp
MapReduce, Hive, Pig, Sqoop, Shell-script, Java, Databases (Oracle & DB2)
 
The main objective of this project is ingesting large amounts of data from traditional source systems (RDBMS, Mainframe and Files) with sqoop into a Hadoop Data Lake after performing different data validations, through Hadoop ecosystem tools like Pig and Hive.
One of Cognizant’s indigenous framework known as ELF (Extract and Load Framework) is majorly used in the ingestion process which is an accumulation of different Big Data technologies. The framework goes through necessary modifications from time to time in accordance with client’s needs.            	            	 
•	Involved in Data profiling/Analysis and metadata generation.
•	Created shell scripts to automate metadata capturing of different sources.  
•	Processing and loading the data files to HDFS and creating Hive Tables on top of it.
•	Debugging errors during Ingestion Job.
•	Used Tivoli scheduler to schedule jobs.
•	Involved in Unit Test case/execution.

Metlife
Hive, Pyspark, Python, Shell-script, Oozie, Tivoli WS, Hue
 
Have worked under the Variable Annuity Dynamic Hedging Projects where the Objective of the project is to forecast and mitigate risks by Analysis for Metlife Corporation. Have worked with the Big Data offshore team for Creating Hive tables and loading them from Oracle Db with Transformations logic being applied to data before loading it into hive. We use python, pyspark, hive, Oozie, Tivoli and shell script to accomplish our task. The project follows Agile model and promotes independent Module development in parallel with team task.
uring Ingestion Job.
•	End to End responsibility of Ingestion.
•   Developed Python script for Validation at different checkpoints throughout the whole process flow.
•	Developed shell script to Run jobs from a non Hadoop linux server to remote Hadoop cluster using webhdfs API and curl command.
• 	Received award from metlife for work done in the first quarter.
• 	Debugging & Unit Tesing
• 	Other ad hoc tasks:
  	o   	UI development using Tkinter (Python’s interface)
  	o   	Developed a demo site using Django for a client pitch  

TECHNICAL SKILLS
Languages: Python and shell scripting, HTML & CSS.
Big Data Ecosystem: Hadoop, PySpark, Map-Reduce, Yarn, Hive, Pig, Sqoop, Oozie.
Database: MySQL, DB2.

AWARDS AND CERTIFICATIONS
•   Have won Award as a Big Data Developer under current project (Metlife).
•	Received ‘Big Data & Hadoop development’ certification by Acadgild, 2017.
Mention any other society memberships, performance appraisals or even college time awards( always the most recent one on top)
 
INTERNSHIP/ INDUSTRIAL TRAINING
Industrial Training at Hewlett Packard Education Services on Android Application
Development, 2015
Developer, Dorf Enterprises, Guna, Madhya Pradesh, feb. – august 2015
 	●    	Created fully functional e-commerce websites.
 	●    	Interacted with customers to discuss sites’ layout, support and maintenance after creation.

EDUCATION
Bachelor of Technology, Computer Science & Engineering, Jaypee University of Engineering &
Technology, Guna, Madhya Pradesh, India, 64 %,  june 2016
 
academic Projects
●	System to analyze log data (In XML format) of government progress of various development activities, 2017
   	Used flume to format and store data in HDFS and then converted data into comma, separated values using MapReduce.
  	Wrote a Pig UDF to filter a particular chunk of data and export the results to MySQL using Sqoop.
●	Worked on project titled ‘Dev Box’, june 2016 – august 2016
  	A SaaS project made with Ruby on Rails. It is a community platform where developers meet entrepreneurs and investors. Users can check out each other’s profile and send messages to connect with each other.
 


